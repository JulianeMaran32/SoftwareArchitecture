# pipeline/logstash.conf

input {
  # Lê os logs do arquivo que a aplicação Spring Boot gera.
  # O caminho corresponde ao volume montado no docker-compose.yml.
  file {
    path => "/usr/share/logstash/app-logs/stock-app.log"
    start_position => "beginning"
    sincedb_path => "/dev/null" # Para fins de teste, sempre relê o arquivo ao iniciar. Remova em produção.
    codec => "json_lines" # Seus logs já são quase JSON, mas precisam ser tratados linha a linha.
  }
}

filter {
  # O Grok é usado para parsear logs de texto plano.
  # O seu padrão de log é: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - [%X{request_id}] - %msg%n"
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{DATA:thread}\] %{LOGLEVEL:level}\s+%{JAVACLASS:class}\s+-\s+\[%{DATA:request_id}\]\s+-\s+%{GREEDYDATA:log_message}" }
    overwrite => [ "message" ] # Substitui o campo 'message' original pela mensagem parseada.
  }

  # Converte o timestamp extraído para o campo principal do evento
  date {
    match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }

  # Remove campos desnecessários
  mutate {
    remove_field => ["log_timestamp", "path", "host", "event"]
  }
}

output {
  # Envia os logs processados para o Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # Cria um índice por dia, o que é uma boa prática para gerenciamento de logs.
    index => "stockapp-logs-%{+YYYY.MM.dd}"
  }
  # (Opcional) Imprime no console do Logstash para depuração
  stdout {
    codec => rubydebug
  }
}