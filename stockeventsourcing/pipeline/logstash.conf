# pipeline/logstash.conf

# ==============================================================================
# ENTRADA (INPUT): De onde os dados vêm?
# ==============================================================================
input {
  # Nós vamos ler de um arquivo.
  file {
    # O caminho para o arquivo de log DENTRO do container do Logstash.
    # No próximo passo, vamos mapear este caminho para o arquivo real.
    path => "/usr/share/logstash/app-logs/stock-app.log"

    # Configurações para desenvolvimento:
    start_position => "beginning"  # Sempre começar a ler do início do arquivo.
    sincedb_path => "/dev/null"    # Não memorizar onde parou de ler (útil para testes).
  }
}

# ==============================================================================
# FILTRO (FILTER): O que fazer com os dados? (A Mágica Acontece Aqui)
# ==============================================================================
filter {
  # O filtro 'grok' é usado para dar estrutura a dados de texto não estruturados.
  grok {
    # 'match' tenta aplicar um padrão à linha de log ('message').
    # Este padrão DEVE corresponder ao 'logging.pattern.console' do seu application.yml.
    match => { "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{DATA:thread}\] %{LOGLEVEL:level}\s+%{JAVACLASS:class}\s+-\s+\[%{DATA:request_id}\]\s+-\s+%{GREEDYDATA:log_message}" }

    # Se o padrão der certo, a linha de log original será sobrescrita.
    overwrite => [ "message" ]
  }

  # O filtro 'date' converte o timestamp que extraímos do log
  # para o campo principal de data do evento (@timestamp).
  # Isso é crucial para o Kibana funcionar corretamente com o tempo.
  date {
    match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }

  # O filtro 'mutate' permite modificar campos. Aqui, vamos remover
  # campos técnicos que não precisamos mais.
  mutate {
    remove_field => ["log_timestamp", "path", "host", "event"]
  }
}

# ==============================================================================
# SAÍDA (OUTPUT): Para onde enviar os dados processados?
# ==============================================================================
output {
  # Vamos enviar para o nosso serviço Elasticsearch.
  elasticsearch {
    # 'elasticsearch' é o nome do serviço definido no docker-compose.yml.
    hosts => ["http://elasticsearch:9200"]

    # Nome do índice onde os logs serão salvos.
    # É uma boa prática criar um índice por dia.
    index => "stockapp-logs-%{+YYYY.MM.dd}"
  }

  # (Opcional, mas muito útil para depuração)
  # Também imprime os logs processados no console do Logstash.
  stdout {
    codec => rubydebug
  }
}